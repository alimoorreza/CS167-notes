{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alimoorreza/CS167-notes/blob/main/Day22_Convolutional_Neural_Network_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHJrUI0I4uVJ"
      },
      "source": [
        "# CS167: Day22\n",
        "## Deep Learning and Convolutional Neural Network (CNN)\n",
        "\n",
        "#### CS167: Machine Learning, Fall 2023\n",
        "\n",
        "Tuesday, 21th, 2023\n",
        "\n",
        "\n",
        "ðŸ“† [Course Schedule](https://analytics.drake.edu/~reza/teaching/cs167_fall23/cs167_schedule.html) | ðŸ“œ [Syllabus](https://analytics.drake.edu/~reza/teaching/cs167_fall23/cs167_syllabus_fall23.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch library\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "LwgBlHEvJDEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## __Put the Model on Training Device (GPU or CPU)__\n",
        "We want to accelerate the training process using graphical processing unit (GPU). Fortunately, in Colab we can access for GPU. You need to enable it from _Runtime-->Change runtime type-->GPU or TPU_"
      ],
      "metadata": {
        "id": "OpvuEwrzrVe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if torch.cuda is available, otherwise it will use CPU\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFa7eTujrik7",
        "outputId": "2e011a83-75a7-4400-ec80-4df2ec7f8472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Load the Dataset for your CNN__"
      ],
      "metadata": {
        "id": "82757Boz0u5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily import some [built-in datasets](https://pytorch.org/vision/stable/datasets.html) from PyTorch's [torchvision.datasets](torchvision.datasets) module\n",
        "- [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
        "  - each image size: 28x28 grayscale image\n",
        "  - each image is associated with a label from __10 classes__\n",
        "  - training set of 60,000 examples and a test set of 10,000 examples\n",
        "\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_fall23/notes/images/fashion-mnist-sprite.png\" width=500/>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "dDGk01A804vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets # torchvision has many deep learning benchmark datasets Fashion-MNIST, CIFAR-10, Caltech-50, etc\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"/content/drive/MyDrive/cs167_fall23/datasets\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor() # specify the feature and label transformations\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"/content/drive/MyDrive/cs167_fall23/datasets\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "4kz7QVYF1ECa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Explore some sample training images__"
      ],
      "metadata": {
        "id": "3rcoqtTu1JH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a random set of images and their the labels from the training split\n",
        "# The following labels represent 10 classes, each with specific indices as defined by the creator of the Fashion-MNIST dataset\n",
        "# reference: https://github.com/zalandoresearch/fashion-mnist#labels\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(5, 5))\n",
        "cols, rows = 5, 2\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    #print('image tensor size:', img.shape)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\") # .squeeze() method removes the '1' from first dimension of the tensor [1, 28, 28]\n",
        "    #print('after removing the first dimension with ', img.squeeze().shape)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rNVp5mR51Lo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Prepare Your Data with DataLoader for Training/Testing__\n",
        "We just explored one sample of data at a time. As we have seen in our discussion of the optimizer, specifically __Stochastic Gradient Descent (SGD)__, during training your network, we may need to pass them in __minibatches__. PyTorch has a module called __DataLoader__, which will do this automatically for us as long as we provide the right arguments:\n",
        "- prepare the __minibatches__ with the given _batch_size_ eg 16, 32, 64, 128, etc\n",
        "- multiprocessing to speed up the data retrieval\n",
        "- reshuffle the data at every __epoch__\n"
      ],
      "metadata": {
        "id": "5w26ZMYs1vL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "#                              pairs of items,   minibatch size,      random shuffling turned ON\n",
        "train_dataloader = DataLoader(training_data,     batch_size=128,        shuffle=True)\n",
        "test_dataloader  = DataLoader(test_data,         batch_size=128,        shuffle=False) # for testing/inference: it is not necessary to shuffle\n",
        "\n",
        "# explore the data from the train_dataloader\n",
        "train_inputs, train_labels = next(iter(train_dataloader)) # returns a batch of 128 train-images and train-labels\n",
        "print(f\"Images batch shape: {train_inputs.shape}\")\n",
        "print(f\"Labels batch shape: {train_labels.shape}\")\n",
        "\n",
        "# visualize one of the samples in this batch of 128\n",
        "figure = plt.figure(figsize=(2, 2))\n",
        "img = train_inputs[127].squeeze() # I picked 127 but you can pick any index in between 0 to batch_size=128\n",
        "label = train_labels[127]         # It's a tensor datatype\n",
        "plt.title(labels_map[int(label)]) # For indexing, convert the 'tensor' datatype --> integer number datatype\n",
        "plt.imshow(img, cmap=\"gray\")\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ],
      "metadata": {
        "id": "x9hHtMpu130p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Building Convolutional Neural Network (CNN)__\n",
        "\n",
        "Create a network class with two methods:\n",
        "- _init()_\n",
        "- _forward()_\n"
      ],
      "metadata": {
        "id": "q4pTsgDsTEiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# You can give any name to your new network, e.g., SimpleCNN.\n",
        "# However, you have to mandatorily inherit from nn.Module to\n",
        "# create your own network class. That way, you can access a lot of\n",
        "# useful methods and attributes from the parent class nn.Module\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # your network layer construction should take place here\n",
        "    # ...\n",
        "    # ...\n",
        "\n",
        "  def forward(self, x):\n",
        "    # your code for MLP forward pass should take place here\n",
        "    # ...\n",
        "    # ...\n",
        "    return x"
      ],
      "metadata": {
        "id": "9DxGu6AUTW10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the CNN as shown in the picture above using this template. In general, we will follow this template for constructing other neural networks such as MLP, CNN, RNN, and Transformer in PyTorch. Hence, it is a very generic setup. Here are the useful PyTorch modules we will be using for CNN construction:\n",
        "- [nn.Conv2d()](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
        "  - applies a 2D convolution over an input volume of $(C_{in}â€‹,H_{in},W_{in})$ and produces an output volume of $(C_{out}â€‹,H_{out},W_{out})$   between two adjacent layers.\n",
        "  - to create this, you need to provide the followings:\n",
        "    - __channel_dimension_of_input_layer__ i.e., $C_{in}$\n",
        "    - __channel_dimension_of_output_layer__ i.e., $C_{out}$\n",
        "    - __filter_size__ i.e., $F$\n",
        "\n",
        "  - the other two optional parameters are __stride__: $S=1$ and __padding__: $P=0$, with default values as shown. As discussed in class, PyTorch will calculate internally the sizes of output volume $H_{out}$ and $W_{out}$ from the above mentioned parameter values\n",
        "\n",
        "<div>\n",
        "<img src=\"https://analytics.drake.edu/~reza/teaching/cs167_fall23/notes/images/convolution-animation-3x3-kernel\" width=800/>\n",
        "</div>\n",
        "\n",
        "[Source of this excellent visualization that vividly illustrates the proces](https://animatedai.github.io/)\n"
      ],
      "metadata": {
        "id": "ww0jFonlZh4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pdb\n",
        "\n",
        "class SimpleCNNv1(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # your network layer construction should take place here\n",
        "\n",
        "    input_volume_channel_first      = 1\n",
        "    output_volume_channel_first     = 32\n",
        "    input_volume_channel_second     = 32\n",
        "    output_volume_channel_second    = 64\n",
        "    num_of_neurons_output_layer     = 10 # usually this number should be equal to the total number of classes in your classification task\n",
        "\n",
        "    # Beginning layers: a series of 2D convolutional layers (useful for feature map learning from the grid layouts of an image)\n",
        "    self.first_conv_2d              = nn.Conv2d(input_volume_channel_first, output_volume_channel_first, 3)  # 2D convolutional transformation module :input_volume_channel=1, output_volume_channel=32, filter_size= (3x3)\n",
        "    self.relu_activation_1st        = nn.ReLU()\n",
        "    self.second_conv_2d             = nn.Conv2d(input_volume_channel_second, output_volume_channel_second, 3) # 2D convolutional transformation module :input_volume_channel=32, output_volume_channel=64, filter_size= (3x3)\n",
        "    self.relu_activation_2nd        = nn.ReLU()\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # -------                 heads up!                                 --------\n",
        "    # you need to calculate the total_size_of_the_output_volume of your self.second_conv_2d layer,\n",
        "    # as it will be needed by the upcoming nn.Linear(). This number will be used as the first argument for the next nn.Linear().\n",
        "    # I pre-calculated this number, and it is 64*24*24 = 36864. I will plug this number in the next layer\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "    # End layers: a series of dense linear layers (almost like an MLP for final classification)\n",
        "    self.first_linear_layer         = nn.Linear(64*24*24, 128)  # linear transformation module (input=64*24*24, output=128)\n",
        "    self.relu_activation_3rd        = nn.ReLU()\n",
        "    self.output_layer               = nn.Linear(128, num_of_neurons_output_layer) # linear transformation module (input=128, output=10)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # your code for Conv_2d forward pass should take place here\n",
        "    output = self.first_conv_2d(x)\n",
        "    output = self.relu_activation_1st(output)\n",
        "    print('first_conv_2d output feature map size: ', output.shape)\n",
        "    output = self.second_conv_2d(output)\n",
        "    output = self.relu_activation_2nd(output)\n",
        "    print('second_conv_2d output feature map size: ', output.shape)\n",
        "\n",
        "    output = torch.flatten(output, 0)\n",
        "\n",
        "    # your code for MLP forward pass should take place here\n",
        "    output = self.first_linear_layer(output)\n",
        "    print('first linear layer feature map shape: ', output.shape)\n",
        "    output = self.relu_activation_3rd(output)\n",
        "    output = self.output_layer(output)\n",
        "    print('Output linear layer feature map shape: ', output.shape)\n",
        "\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "HNuxDcIFYjY6"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the structure of your MLP\n",
        "cnn_model = SimpleCNNv1()\n",
        "print(cnn_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlwRblRgmVlv",
        "outputId": "1ee65555-ee94-482e-bb2c-35503310e806"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNNv1(\n",
            "  (first_conv_2d): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (relu_activation_1st): ReLU()\n",
            "  (second_conv_2d): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (relu_activation_2nd): ReLU()\n",
            "  (first_linear_layer): Linear(in_features=36864, out_features=128, bias=True)\n",
            "  (relu_activation_3rd): ReLU()\n",
            "  (output_layer): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the sizes of weights and biases of your MLP's 1st hidden layers\n",
        "print('size of weights of first_hidden_layer: \\n ', cnn_model.first_conv_2d.weight.shape)\n",
        "print('size of bias of first_hidden_layer: \\n ', cnn_model.first_conv_2d.bias.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q0AlCh4vcDh",
        "outputId": "03c28faf-e317-48e5-e245-bf415e463019"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of weights of first_hidden_layer: \n",
            "  torch.Size([32, 1, 3, 3])\n",
            "size of bias of first_hidden_layer: \n",
            "  torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check the randomly initialized values of weights and biases of your MLP's 1st hidden layers\n",
        "print('weights of first_conv_2d.weight: \\n ', cnn_model.first_conv_2d.weight)\n",
        "print('bias of first_conv_2d.weight: \\n ', cnn_model.first_conv_2d.bias)"
      ],
      "metadata": {
        "id": "8-WYre5Fu5qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Forward Pass using your Dataset and your CNN__\n",
        "Test a forward pass of our first CNN using one of the training samples.\n",
        "The forward method inside our network class, __SimpleCNNv1__, will be invoked if we provide an input tensor __X__ to the network object we instantiated earlier, i.e., __cnn_model__, as follows:\n",
        "- _output = cnn_model(X)_\n",
        "\n",
        "Finally, we convert the ouput from the model into probabilities using __Softmax()__ module:\n",
        "- _pred_probab = softmax_activation(output)_\n",
        "\n"
      ],
      "metadata": {
        "id": "HGToEDVFl9rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img   = train_inputs[100] # I picked 100 but you can pick any index in between 0 to batch_size=128\n",
        "label = train_labels[100]\n",
        "\n",
        "softmax_activation = nn.Softmax(dim=0)\n",
        "\n",
        "print('shape of X: ', img.shape)\n",
        "output      = cnn_model(img)                # last layer of our network will return 10 values each will range in between in [-infty, infty]\n",
        "pred_probab = softmax_activation(output)   # these raw numbers scaled to values [0, 1] representing the modelâ€™s predicted probabilities for each class\n",
        "print('predited probability \\n', pred_probab)\n",
        "y_pred      = pred_probab.argmax()\n",
        "print(f\"Predicted class: {y_pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OQed_nsazcw",
        "outputId": "21526ff0-4a96-435a-ba9f-86b041a9ad9b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of X:  torch.Size([1, 28, 28])\n",
            "first_conv_2d output feature map size:  torch.Size([32, 26, 26])\n",
            "second_conv_2d output feature map size:  torch.Size([64, 24, 24])\n",
            "first linear layer feature map shape:  torch.Size([128])\n",
            "Output linear layer feature map shape:  torch.Size([10])\n",
            "predited probability \n",
            " tensor([0.1045, 0.0968, 0.1049, 0.1040, 0.1037, 0.0956, 0.0984, 0.1012, 0.0988,\n",
            "        0.0920], grad_fn=<SoftmaxBackward0>)\n",
            "Predicted class: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model parameter counts to find out how many parameter this model will learn from the data\n",
        "for name, param in cnn_model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} \\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCqis2b5kU8A",
        "outputId": "3aeb077b-5222-4397-841a-58aecf46f037"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: first_conv_2d.weight | Size: torch.Size([32, 1, 3, 3]) \n",
            "\n",
            "Layer: first_conv_2d.bias | Size: torch.Size([32]) \n",
            "\n",
            "Layer: second_conv_2d.weight | Size: torch.Size([64, 32, 3, 3]) \n",
            "\n",
            "Layer: second_conv_2d.bias | Size: torch.Size([64]) \n",
            "\n",
            "Layer: first_linear_layer.weight | Size: torch.Size([128, 36864]) \n",
            "\n",
            "Layer: first_linear_layer.bias | Size: torch.Size([128]) \n",
            "\n",
            "Layer: output_layer.weight | Size: torch.Size([10, 128]) \n",
            "\n",
            "Layer: output_layer.bias | Size: torch.Size([10]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Defining Loss function__\n",
        "\n",
        "- [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
        "  - useful when training a __classification problem__ with __C__ classes.\n",
        "  - criterion computes the cross entropy loss between input logits and target\n",
        "- [nn.MSELoss()](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\n",
        "  - useful when training a __regression problem__\n",
        "  - criterion that measures the mean squared error (squared L2 norm) between each element in the input _x_ and target _y_\n"
      ],
      "metadata": {
        "id": "f1gTfGDJm-2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss() # this is useful for multiclass classification task"
      ],
      "metadata": {
        "id": "OK1bAAANnLMz"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##__Initializing the Optimizer__\n",
        "\n",
        "Optimiztaion, as we have discussed in previous week, is process of adjusting model parameters to reduce model error in each training step. PyTorch provides a selection of optimization algorithms in the [torch.optim](https://pytorch.org/docs/stable/optim.html) package. Some of them are as follows:\n",
        "- [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)\n",
        "- [torch.optim..Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
        "- [torch.optim.RMSprop](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html#torch.optim.RMSprop)\n",
        "\n",
        "In addition to selecting the optimizer, we can also select the yperparameters which are refered to as adjustable parameters crucial for controlling the model optimization process. You can influence the training and convergence of the model by tweaking these hyperparameters:\n",
        "- __epochs:__ denotes the number of iterations over the dataset\n",
        "- __batch size:__ represents the quantity of data samples in each iteration propagated through the network before updating the parameters\n",
        "- __learning rate:__ determines the extent of parameter updates made at each batch/epoch\n",
        "\n"
      ],
      "metadata": {
        "id": "J6Mwh6U1mGev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size    = 64\n",
        "epochs        = 10\n",
        "# let's use SGD optimization algorithm for training our model\n",
        "optimizer     = torch.optim.SGD(cnn_model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "11-Lj-avlzFo"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Simpliefy the CNN Network with PyTorch's Sequential() Module__"
      ],
      "metadata": {
        "id": "pIDHTABd4wU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pdb\n",
        "\n",
        "class SimpleCNNv2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # your network layer construction should take place here\n",
        "\n",
        "    input_volume_channel_first      = 1\n",
        "    output_volume_channel_first     = 32\n",
        "    input_volume_channel_second     = 32\n",
        "    output_volume_channel_second    = 64\n",
        "    num_of_neurons_output_layer     = 10 # usually this number should be equal to the total number of classes in your classification task\n",
        "\n",
        "    # Beginning layers: a series of 2D convolutional layers (useful for feature map learning from the grid layouts of an image)\n",
        "    self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_volume_channel_first, output_volume_channel_first, 3),  # 2D convolutional transformation module :input_volume_channel=3, output_volume_channel=32, filter_size= (3x3)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(input_volume_channel_second, output_volume_channel_second, 3), # 2D convolutional transformation module :input_volume_channel=32, output_volume_channel=64, filter_size= (3x3)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    # -------                 heads up!                                 --------\n",
        "    # you need to calculate the total_size_of_the_output_volume of your self.second_conv_2d layer,\n",
        "    # as it will be needed by the upcoming nn.Linear(). This number will be used as the first argument for the next nn.Linear().\n",
        "    # I pre-calculated this number, and it is 64*24*24 = 36864. I will plug this number in the next layer\n",
        "    # --------------------------------------------------------------------------\n",
        "    self.flatten                    = nn.Flatten()\n",
        "\n",
        "    # End layers: a series of dense linear layers (almost like an MLP for final classification)\n",
        "    self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(64*24*24, 128),  # linear transformation module (input=64*24*24, output=128)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_of_neurons_output_layer) # linear transformation module (input=128, output=10)\n",
        "        )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # your code for Conv_2d forward pass should take place here\n",
        "    output = self.conv_layers(x)\n",
        "    output = self.flatten(output)\n",
        "    output = self.linear_layers(output)\n",
        "    return output\n",
        "\n",
        "cnn_model = SimpleCNNv2()\n",
        "print(cnn_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js7bNs2y9HZA",
        "outputId": "982f3ac2-9ab8-4238-cd0a-2fd9fcedb61f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNNv2(\n",
            "  (conv_layers): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Linear(in_features=36864, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Putting Everything Together CNN__"
      ],
      "metadata": {
        "id": "Z7yBVqvGmQM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Putting Everything Together using our SimpleCNNv2 Network on Fashion-MNIST Dataset__\n"
      ],
      "metadata": {
        "id": "Xn0KYm8r1i1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: load the Torch library and other utilities\n",
        "#----------------------------------------------------\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 2: load the dataset, ie, we are experimenting with Fashion-MNIST\n",
        "#--------------------------------------------------------------------------------------------------\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"fashion_mnist\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"fashion_mnist\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "# Step 3: Create your CNN Network (call it SimpleCNNv2) with 2 conv_2d layers + 2 layers of MLP\n",
        "#--------------------------------------------------------------------------------------------------\n",
        "class SimpleCNNv2(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # your network layer construction should take place here\n",
        "\n",
        "    input_volume_channel_first      = 1\n",
        "    output_volume_channel_first     = 32\n",
        "    input_volume_channel_second     = 32\n",
        "    output_volume_channel_second    = 64\n",
        "    num_of_neurons_output_layer     = 10 # usually this number should be equal to the total number of classes in your classification task\n",
        "\n",
        "    # Beginning layers: a series of 2D convolutional layers (useful for feature map learning from the grid layouts of an image)\n",
        "    self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(input_volume_channel_first, output_volume_channel_first, 3),  # 2D convolutional transformation module :input_volume_channel=3, output_volume_channel=32, filter_size= (3x3)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(input_volume_channel_second, output_volume_channel_second, 3), # 2D convolutional transformation module :input_volume_channel=32, output_volume_channel=64, filter_size= (3x3)\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    # --------------------------------------------------------------------------\n",
        "    # -------                 heads up!                                 --------\n",
        "    # you need to calculate the total_size_of_the_output_volume of your self.second_conv_2d layer,\n",
        "    # as it will be needed by the upcoming nn.Linear(). This number will be used as the first argument for the next nn.Linear().\n",
        "    # I pre-calculated this number, and it is 64*24*24 = 36864. I will plug this number in the next layer\n",
        "    # --------------------------------------------------------------------------\n",
        "\n",
        "    self.flatten                    = nn.Flatten()\n",
        "\n",
        "    # End layers: a series of dense linear layers (almost like an MLP for final classification)\n",
        "    self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(64*24*24, 128),  # linear transformation module (input=64*24*24, output=128)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_of_neurons_output_layer) # linear transformation module (input=128, output=10)\n",
        "        )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # your code for Conv_2d forward pass should take place here\n",
        "    output = self.conv_layers(x)\n",
        "    output = self.flatten(output)\n",
        "    output = self.linear_layers(output)\n",
        "\n",
        "    return output\n",
        "\n",
        "cnn_model = SimpleCNNv2()\n",
        "print(cnn_model)\n",
        "\n",
        "# Step 4: Your training and testing functions (you can copy it from the previous demo)\n",
        "#--------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    model.train()                   # set the model to training mode for best practices\n",
        "\n",
        "    train_loss = 0\n",
        "    correct    = 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "            # compute the accuracy\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    return train_loss/len(dataloader), 100*correct\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval()                    # set the model to evaluation mode for best practices\n",
        "\n",
        "    size        = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss, 100*correct\n",
        "\n",
        "# Step 5: prepare the DataLoader and select your optimizer and set the hyper-parameters for learning the model from DataLoader\n",
        "#------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size_val = 64\n",
        "epochs = 10\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(cnn_model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size_val)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size_val)\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "test_losses  = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    avg_train_loss, train_accuracy = train_loop(train_dataloader, cnn_model, loss_fn, optimizer)\n",
        "    avg_test_loss, test_accuracy  = test_loop(test_dataloader, cnn_model, loss_fn)\n",
        "    # save the losses and accuracies\n",
        "    train_losses.append(avg_train_loss)\n",
        "    test_losses.append(avg_test_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "print(\"SimpleCNNv2 model has been trained!\")\n",
        "\n",
        "# visualizing the loss curves\n",
        "plt.plot(range(1,epochs+1), train_losses)\n",
        "plt.plot(range(1,epochs+1), test_losses)\n",
        "plt.title('Model average losses after each epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TE6r3j-71vjK",
        "outputId": "b8dffb69-9f9e-42d6-9ad9-5f12429eca9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNNv2(\n",
            "  (conv_layers): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Linear(in_features=36864, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302379  [   64/60000]\n",
            "loss: 2.199876  [ 6464/60000]\n",
            "loss: 1.983157  [12864/60000]\n",
            "loss: 1.742227  [19264/60000]\n",
            "loss: 1.346542  [25664/60000]\n",
            "loss: 1.095899  [32064/60000]\n",
            "loss: 1.022684  [38464/60000]\n",
            "loss: 0.839562  [44864/60000]\n",
            "loss: 0.867055  [51264/60000]\n",
            "loss: 0.856059  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.4%, Avg loss: 0.779309 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.750698  [   64/60000]\n",
            "loss: 0.807368  [ 6464/60000]\n",
            "loss: 0.539248  [12864/60000]\n",
            "loss: 0.822183  [19264/60000]\n",
            "loss: 0.721650  [25664/60000]\n",
            "loss: 0.716041  [32064/60000]\n",
            "loss: 0.746725  [38464/60000]\n",
            "loss: 0.646953  [44864/60000]\n",
            "loss: 0.706258  [51264/60000]\n",
            "loss: 0.704942  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.4%, Avg loss: 0.655056 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.584273  [   64/60000]\n",
            "loss: 0.662943  [ 6464/60000]\n",
            "loss: 0.441589  [12864/60000]\n",
            "loss: 0.729065  [19264/60000]\n",
            "loss: 0.657321  [25664/60000]\n",
            "loss: 0.655643  [32064/60000]\n",
            "loss: 0.670572  [38464/60000]\n",
            "loss: 0.618209  [44864/60000]\n",
            "loss: 0.699620  [51264/60000]\n",
            "loss: 0.624941  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.8%, Avg loss: 0.623078 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.547566  [   64/60000]\n",
            "loss: 0.607234  [ 6464/60000]\n",
            "loss: 0.411268  [12864/60000]\n",
            "loss: 0.684051  [19264/60000]\n",
            "loss: 0.631822  [25664/60000]\n",
            "loss: 0.621056  [32064/60000]\n",
            "loss: 0.632096  [38464/60000]\n",
            "loss: 0.611591  [44864/60000]\n",
            "loss: 0.705528  [51264/60000]\n",
            "loss: 0.581612  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.1%, Avg loss: 0.607789 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.532001  [   64/60000]\n",
            "loss: 0.579190  [ 6464/60000]\n",
            "loss: 0.391335  [12864/60000]\n",
            "loss: 0.653470  [19264/60000]\n",
            "loss: 0.610341  [25664/60000]\n",
            "loss: 0.596184  [32064/60000]\n",
            "loss: 0.603868  [38464/60000]\n",
            "loss: 0.610641  [44864/60000]\n",
            "loss: 0.706265  [51264/60000]\n",
            "loss: 0.550787  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.3%, Avg loss: 0.599053 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 0.523801  [   64/60000]\n",
            "loss: 0.562103  [ 6464/60000]\n",
            "loss: 0.377216  [12864/60000]\n",
            "loss: 0.625219  [19264/60000]\n",
            "loss: 0.595033  [25664/60000]\n",
            "loss: 0.569103  [32064/60000]\n",
            "loss: 0.582466  [38464/60000]\n",
            "loss: 0.608581  [44864/60000]\n",
            "loss: 0.701894  [51264/60000]\n",
            "loss: 0.532555  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.594564 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 0.521516  [   64/60000]\n",
            "loss: 0.546431  [ 6464/60000]\n",
            "loss: 0.367277  [12864/60000]\n",
            "loss: 0.604576  [19264/60000]\n",
            "loss: 0.576677  [25664/60000]\n",
            "loss: 0.547198  [32064/60000]\n",
            "loss: 0.565819  [38464/60000]\n",
            "loss: 0.606603  [44864/60000]\n",
            "loss: 0.700903  [51264/60000]\n",
            "loss: 0.517539  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.4%, Avg loss: 0.591899 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 0.522417  [   64/60000]\n",
            "loss: 0.533470  [ 6464/60000]\n",
            "loss: 0.357997  [12864/60000]\n",
            "loss: 0.584275  [19264/60000]\n",
            "loss: 0.560185  [25664/60000]\n",
            "loss: 0.527725  [32064/60000]\n",
            "loss: 0.551420  [38464/60000]\n",
            "loss: 0.604032  [44864/60000]\n",
            "loss: 0.696929  [51264/60000]\n",
            "loss: 0.502792  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 77.6%, Avg loss: 0.588067 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.522087  [   64/60000]\n",
            "loss: 0.522081  [ 6464/60000]\n",
            "loss: 0.349913  [12864/60000]\n",
            "loss: 0.567873  [19264/60000]\n",
            "loss: 0.543872  [25664/60000]\n",
            "loss: 0.510898  [32064/60000]\n",
            "loss: 0.538348  [38464/60000]\n",
            "loss: 0.600228  [44864/60000]\n",
            "loss: 0.692888  [51264/60000]\n",
            "loss: 0.492475  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.0%, Avg loss: 0.581733 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.520577  [   64/60000]\n",
            "loss: 0.511352  [ 6464/60000]\n",
            "loss: 0.342784  [12864/60000]\n",
            "loss: 0.553401  [19264/60000]\n",
            "loss: 0.528260  [25664/60000]\n",
            "loss: 0.497711  [32064/60000]\n",
            "loss: 0.526610  [38464/60000]\n",
            "loss: 0.599684  [44864/60000]\n",
            "loss: 0.689741  [51264/60000]\n",
            "loss: 0.480739  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 78.1%, Avg loss: 0.576718 \n",
            "\n",
            "SimpleCNNv2 model has been trained!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing the accuracy curves\n",
        "plt.plot(range(1,epochs+1), '-', label=train_accuracies)\n",
        "plt.plot(range(1,epochs+1), '*', label=test_accuracies)\n",
        "plt.title('Model accuracies after each epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'test'])\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9yAt25QEQx1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#__Group Exercise__:\n",
        "Create another CNN with 3 layers of 2D convolution and then train that model on the Fashion-MNIST dataset using the SGD optimizer with a batch size of 64. If time permits, you can also train another CNN with the ADAM optimizer and observe if there is any change in training."
      ],
      "metadata": {
        "id": "p9MmCZ0--f9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: load the Torch library and other utilities\n",
        "#----------------------------------------------------\n"
      ],
      "metadata": {
        "id": "SVonqhsEQXl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: load the dataset, ie, we are experimenting with Fashion-MNIST\n",
        "#--------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "kqJB4hncQdXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create your CNN Network (call it SimpleCNNv3) with 3 conv_2d layers\n",
        "#--------------------------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "NEzPNtrrQfeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Your training and testing functions (I copied it previous the previous demo. You don't need to change anything)\n",
        "#--------------------------------------------------------------------------------------\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "\n",
        "    size = len(dataloader.dataset)\n",
        "\n",
        "    model.train()                   # set the model to training mode for best practices\n",
        "\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "    return train_loss/len(dataloader)\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "\n",
        "    model.eval()                    # set the model to evaluation mode for best practices\n",
        "\n",
        "    size        = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return test_loss\n"
      ],
      "metadata": {
        "id": "Yw6a5TByQklU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: prepare the DataLoader and select your optimizer and set the hyper-parameters for learning the model from DataLoader\n",
        "#------------------------------------------------------------------------------------------------------------------------------\n",
        "# Initialize your hyper-parameter and optimizer with appropriate values\n",
        "# Use your dataset to prepare your DataLoaders one for training_data and one for test_data\n",
        "# ...\n",
        "# ...\n",
        "\n",
        "print(\"SimpleCNNv3 model has been trained!\")\n"
      ],
      "metadata": {
        "id": "tejagPCKQztY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualizing the loss curves\n"
      ],
      "metadata": {
        "id": "_0hoLrbIRQtk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}